{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 100)\n",
    "pd.set_option(\"max_colwidth\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Corona_NLP_train.csv\")\n",
    "df_test = pd.read_csv(\"./data/Corona_NLP_test.csv\")\n",
    "#df = dd.from_pandas(df, npartitions=1)\n",
    "#df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-recycling",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "Let's begin with examining a few datapoints and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-madonna",
   "metadata": {},
   "source": [
    "There are some immediate observations about the tweets: \n",
    "\n",
    "Obviously there are hashstags, which are concatenations of words, which might not be able to be tokenized easily. Hashtags could, however be extracted as a separate feature.\n",
    "\n",
    "Some tweets contain URL, which might not be easily processed within standard NLP pipelines.\n",
    "\n",
    "Some tweets contain tab and newline characters ('\\n' or '\\r').\n",
    "\n",
    "The location attributes seems not to correspond to physical information in a lot of cases. If this attribute was to be used, there would be a need for complex data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-vault",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(df.Location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-contractor",
   "metadata": {},
   "source": [
    "Furthermore, the location is missing in an approximate 20% of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"Sentiment\", order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-network",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Next let's identify potentially problematic patterns using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-philosophy",
   "metadata": {},
   "source": [
    "Clean up whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace('[\\n\\r]', ' ', regex=True)\n",
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace(' +', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-change",
   "metadata": {},
   "source": [
    "Remove repeated question marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace('\\?+', '?', regex=True)\n",
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace('(?:\\? ?)+', '?', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-spain",
   "metadata": {},
   "source": [
    "Remove URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace('http[^ ]*', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-punishment",
   "metadata": {},
   "source": [
    "Change ampercent sign and the xml entity to \"and\" word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace('(&amp;)|&', 'and', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-stick",
   "metadata": {},
   "source": [
    "Change hashtags to normal words (remove the '#').\n",
    "\n",
    "'#yolo' -> 'yolo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace('#', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-training",
   "metadata": {},
   "source": [
    "Remove reference to twitter Users.\n",
    "\n",
    "Example: '@bbc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['OriginalTweet'] = clean_df['OriginalTweet'].str.replace('@[^ ]+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[~clean_df['OriginalTweet'].str.contains('\\w{3,}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-nepal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
